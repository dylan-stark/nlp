Stark Response: a Shiny App for Creatives
========================================================
author: Dylan Stark
date: `r Sys.Date()`
autosize: true


Stuck? Can't think of the next ... word?
========================================================

<br/>

> The **Stark Response** app is your off-board brain.

<br/>

Stay in the flow when composing your next witty response, whether it's a tweet, blog post, or news-worthy article.

Just start crafting your message in the simple text box and look to the beautiful word cloud response for inspiration.

The advanced machine learning algorithm will show you a bounty of options!

Find inspiration while staring at the clouds
========================================================

> The **Stark Response** app is intuitive.

To paraphrase Dory, "**Just keep typing. Just keep typing. Just keep typing**"!

<div align="center">
<img src="stark_response_good.jpg" width=312 height=383>
<img src="stark_response_thank.jpg" width=312 height=383>
<img src="stark_response_thank_you.jpg" width=312 height=383>
</div>

Stop worrying and learn to love the AI
========================================================

> The **Stark Response** app is powered by machine learning.

1. Ingested huge amounts of Twitter, blog, and news, so you did't have to
1. Constructed a multiple $n$-gram language model with support for
  * out-of-vocab words,
  * [Kneser-Ney smoothing](https://en.wikipedia.org/wiki/Kneserâ€“Ney_smoothing) -- $p_{KN}(w_i | w_{i-3+1}^{i-1})$, with
  * backoff from trigrams to unigrams
1. Delivered a quick, efficient, and effective prediciton with [Markov modeling](https://en.wikipedia.org/wiki/Markov_model)

**All of which adds up to never being left with nothing to say!**

But, don't just take our words for it
========================================================

Go ahead and kick the tires:

> The **Stark Response** app is online!

Try it out: [`https://dstark.shinyapps.io/stark_response_v01/`](https://dstark.shinyapps.io/stark_response_v01/)

## Special Thanks

[Dan Jurafsky](http://web.stanford.edu/~jurafsky/)'s Speech and Language Processing book, esp. the [Language Modeling and N-Grams](https://web.stanford.edu/~jurafsky/slp3/4.pdf) were the primary source for the language models and Kneser-Ney smoothing

[Julia Silge](http://juliasilge.com) and [David Robinson](http://varianceexplained.org)'s [Text Mining with R](http://tidytextmining.com) and the [tidytext package](https://github.com/juliasilge/tidytext) were the primary tools used for implementing the models.

