---
title: "Milestone Report"
author: "Dylan Stark"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Milestone Report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.asp = 0.618, fig.align = "center", fig.width = 6,
                      out.width = "70%")
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(stringr)
library(tidytext)
library(ggthemes)

library(courseraswiftkey)

theme_set(theme_tufte())

data("en_us_news")
data("en_us_blogs")
data("en_us_twitter")
```

## Summary

Technology has scaled down to make smartphones, smartwatches, and small tablets the primary way that people connect to the internet and to each other.
Unfortunately, communication has not scaled down as well and typing, key-by-key, is still the primary means for communicating.
That means we need a way to improve the user experience when communicating with text.
This project will address this issue by providing the user with an augmented keyboard that saves them from typing every word by allowing them to choose whole words without having to start typing them.

The implementation will be based on learning language patterns and applying those to accurately predict, or recommend, follow-on words.
The explosion of electronic media and publicly available large datasets means the data we need to train our models on is readily available.
And by using [probabilistic n-gram models]() we can develop an application that is quick enough to keep the user happy and small enough to fit on the smallest of devices.

The remainder of this report describes a single source of English text across three genres that will serve as the basis for the application prototype.
To goal of the preliminary investigation is to look at trends in the data which will influence our application design.
Three takeaways from the the data are

1. Users writing news, blogs, or twitter messages have considerably different needs.
1. The data is sparse -- we will need to [smooth]()
1. Even within genre, the OOV rate is high

The initial prototype application will involve developing a straightforward [trigram model]() per genre with [backoff]() which meet our spec. processing and memory requiremens.
From this we will explore options for improving accuracy while honoring the resource bounds, making sure we do end up with a highly accurate model that needs a too much memory or processing power to run on a resource-constrained device.
The target environment for the early prototype is a Shiny app that will execute in a resource-constrained environment.
This means we cannot rely on having lots of memory or cores for loading and processing large data sets.
We should also not expect to be able to run many models in parallel, as we might do for a random forests, for instance.

## Sources and Genres

The English corpora from the [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) is used in this report.
The table below shows details for each source: blogs, news, and twitter.
Given the amount of data for each source we decided to take a sub-sample for this exploration.

```{r}
file_line_count <- function(filename) {
  paste0("wc -l ", filename) %>%
    system(intern = TRUE) %>%
    str_extract("\\d+") %>%
    as.integer()
}
```

```{r files_data, cache=TRUE}
files <- tribble(
  ~ raw_filename, ~ rda_filename,
  "../data-raw/final/en_US/en_US.blogs.txt", "../data/en_us_blogs.rda",
  "../data-raw/final/en_US/en_US.news.txt", "../data/en_us_news.rda",
  "../data-raw/final/en_US/en_US.twitter.txt", "../data/en_us_twitter.rda"
) %>%
  mutate(raw_size = file.size(raw_filename),
         raw_line_count = map_int(raw_filename, file_line_count),
         rda_size = file.size(rda_filename))
```

```{r raw_data_table}
files %>%
  select(-rda_filename) %>%
  mutate(raw_filename = str_replace(raw_filename, "../data-raw/final/en_US/", "")) %>%
  rename(`File name` = raw_filename,
         `File size (B)` = raw_size,
         `Line count` = raw_line_count,
         `Sub-sample size (B)` = rda_size) %>%
  knitr::kable(caption = "Raw file information. Sub-sample size is in Rda format.",
               format.args = list(big.mark = ","))
```

```{r texts_data, cache=TRUE, dependson=c("files_data")}
texts <- tribble(
  ~ source, ~ corpora,
  "Blogs", en_us_blogs,
  "News", en_us_news,
  "Twitter", en_us_twitter
) %>%
  mutate(num_obs = map_int(corpora, nrow),
         words = map(corpora, ~ unnest_tokens(.x, word, text)),
         total_words = map_int(words, nrow),
         unique_words = map_int(words, ~ length(unique(.x[["word"]]))))
```

The following table shows more statistics for each of the sub-sampled sources.
One point to note is that since we sample lines the Twitter data set has less than half the number of total words and just around half the number of distinct words as the news and blogs source, which makes sense considering the limited number of characters allowed in a tweet.

```{r text_data_table}
texts %>%
  select(-corpora, -words) %>%
  knitr::kable(col.names = c("Source", "Line count", "Total word count", "Distinct word count"),
               format.args = list(big.mark = ","))
```

These sizes demonstrate that a look-up approach is untenable and that we need a space-efficient model.

```{r}
texts <- tribble(
  ~ source, ~ corpora,
  "Blogs", en_us_blogs,
  "News", en_us_news,
  "Twitter", en_us_twitter
) %>%
  mutate(split = map(corpora, ~ enframe(modelr::resample_partition(.x, p = c(train = 0.80, validate = 0.10, test = 0.10)))  %>% spread(name, value))) %>%
  unnest(split) %>%
  mutate(words = map(train, ~ unnest_tokens(as.data.frame(.x), word, text)))
```

## Frequent words

As we might expect, only a relatively small number number or words are used frequently and most are words are used very infrequently.
The following figure shows the frequency distribution of words with respect to each source.
Note that the distributions for each source are very similar.

```{r cache=TRUE, dependson=c("texts_data")}
texts %>%
  select(source, words) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  ggplot(aes(n)) +
  geom_density(aes(color = source)) +
  scale_x_log10() +
  labs(title = "Word frequency distributions are extremely left-skewed",
       x = "", y = "")
```

Given this distribution, it might be beneficial to look at the top-25 most frequent words per source.
The following figure combines the top-25 lists for each genre of text and shows the proportion of the sources that each word accounts for.
The words are ordered based on the ranking for news, allowing us to compare that trend with the other sources.
At the top of the list for all three sources are the expected words like "the", "and", "a", etc.
However, what this side-by-side comparison helps highlight that while the frequency distributions for each source are very similar, the most frequent words are not.
Both blogs and Twitter text have significantly higher use of personal pronouns, such as "I" and "you", than are seen in news.
This is intuitive, that blogs and twitter are more personal and direct communication mediums, and a good indicator that we should model these different sources differently.

```{r cache=TRUE, dependson=c("texts_data")}
top_words <- texts %>%
  select(source, words) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  top_n(25, n) %>%
  ungroup() %>%
  distinct(word)

texts %>%
  select(source, words) %>%
  mutate(source = factor(source, levels = c("News", "Blogs", "Twitter"), ordered = TRUE)) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  mutate(prop = n / sum(n)) %>%
  inner_join(top_words, by = "word") %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, prop)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  facet_wrap(~ source) +
  labs(title = "Per-source proportion of top-25 most frequent words",
       x = "", y = "Proportion of text")
```

The following table shows another indication of the cross-genre differences.
Here we look at the correlation of word use.
We see that the news and blog corpora are most alike and that the news and twitter corpora for are least similar.

```{r}
tidy_freqs <- texts %>%
  select(source, words) %>%
  mutate(source = factor(source, levels = c("News", "Blogs", "Twitter"), ordered = TRUE)) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  mutate(frequency = n / sum(n)) %>%
  ungroup() %>%
  filter(str_detect(word, "['a-z]+")) %>%
  select(-n)
```

```{r}
cor_news_blogs <- tidy_freqs %>%
  spread(source, frequency) %>%
  cor.test(~ News + Blogs, data = ., na.action = "na.omit")
```

```{r}
cor_news_twitter <- tidy_freqs %>%
  spread(source, frequency) %>%
  cor.test(~ News + Twitter, data = ., na.action = "na.omit")
```

```{r}
cor_blogs_twitter <- tidy_freqs %>%
  spread(source, frequency) %>%
  cor.test(~ Blogs + Twitter, data = ., na.action = "na.omit")
```

```{r}
bind_rows(
  mutate(broom::tidy(cor_news_blogs), pair = "News-Blogs"),
  mutate(broom::tidy(cor_news_twitter), pair = "News-Twitter"),
  mutate(broom::tidy(cor_blogs_twitter), pair = "Blogs-Twitter")
) %>%
  select(pair, estimate, conf.low, conf.high) %>%
  arrange(desc(estimate)) %>%
  knitr::kable(digits = 3, col.names = c("Correlation Test", "Estmiate", "Lower 95% CI", "Upper 95% CI"))
```

## Sparse word pairs

The previous section looked at the distributions of words and how the different genres differ at the word level.
We now want to investigate word pairings, or bigrams.
Of particular interest is understanding how sparse the data set is as this will effect both how we handle smoothing of the data and how we represent the model in a space-efficient manner.

This figure shows which of all possible pairs of the top-25 words from the News set are connected.
Even given that these are mostly common articles and conjunctions we see the data set is rather sparse.

```{r}
top_news_words <- texts %>%
  filter(source == "News") %>%
  select(words) %>%
  unnest(words) %>%
  count(word, sort = TRUE) %>%
  top_n(25, n)

news_bigrams <- texts %>%
  filter(source == "News") %>%
  select(corpora) %>%
  unnest(corpora) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("left", "right"), sep = " ")

news_bigrams %>%
  filter(left %in% top_news_words$word) %>%
  filter(right %in% top_news_words$word) %>%
  ggplot(aes(left, right)) +
  geom_tile(show.legend = FALSE) +
  coord_flip() +
  labs(x = "First word", y = "Second word") +
  theme(axis.ticks = element_blank())
```

The next figure "zooms out" to show which pairs exist for (only) the top-500 words in the News source.
In this case, we show the adjacency matrices for each genre.

```{r}
top_news_words <- texts %>%
  filter(source == "News") %>%
  select(words) %>%
  unnest(words) %>%
  count(word, sort = TRUE) %>%
  top_n(500, n)

all_bigrams <- texts %>%
  select(source, corpora) %>%
  unnest(corpora) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(source, bigram, sort = TRUE) %>%
  separate(bigram, c("left", "right"), sep = " ")

all_bigrams %>%
  filter(left %in% top_news_words$word) %>%
  filter(right %in% top_news_words$word) %>%
  ggplot(aes(left, right, fill = source)) +
  geom_tile() +
  facet_wrap(~ source) +
  labs(title = "Very, very sparse",
       x = "First word", y = "Second word") +
  coord_flip() +
  guides(fill = FALSE) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank())
```

From this it's clear that we have high sparsity in each genre.

## Unknown words

We now assess the out-of-vocabulary (OOV) rate.
The corpora for each genre was split: 80% for training, 10% for validation, and 10% for the final model testing.
An OOV set was computed as the set of unique words in the validation dataset but not in the training dataset.
From this we computed the OOV rate as the percentage of OOV in the validation dataset.
The following table shows the rates for each genre.

```{r}
oov <- texts %>%
  mutate(unique_train_words = map_int(words, ~ length(unique(.x[["word"]]))),
         validate_words = map(validate, ~ unnest_tokens(as.data.frame(.x), word, text)),
         unique_validate_words = map_int(validate_words, ~ length(unique(.x[["word"]]))),
         oov = setdiff(unique_validate_words, unique_train_words),
         oov_rate  = length(oov) / unique_validate_words)
oov <- texts %>%
  select(-corpora, -test) %>%
  mutate(validate_words = map(validate, ~ unnest_tokens(as.data.frame(.x), word, text)),
         distinct_train_words = map(words, ~ distinct(.x, word)),
         distinct_validate_words = map(validate_words, ~ distinct(.x, word)),
         oov_words = map2(distinct_validate_words, distinct_train_words, anti_join, by = "word"),
         oov_rate = map_int(oov_words, nrow) / map_int(distinct_validate_words, nrow))
```

```{r}
oov %>%
  select(source, oov_rate) %>%
  knitr::kable(digits = 3, col.names = c("Source", "OOV Rate"))
```

As we see, even in the relatively small hold-out, around 25% of the words do not exist in the training data.
That means it's likely that our application will have to handle a significant number of words that were not used when training the model.
