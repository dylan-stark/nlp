---
title: "Milestone Report"
author: "Dylan Stark"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Milestone Report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.asp = 0.618, fig.align = "center", fig.width = 6,
                      out.width = "70%")
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(stringr)
library(tidytext)
library(ggthemes)

library(courseraswiftkey)

theme_set(theme_tufte())

data("en_us_news")
data("en_us_blogs")
data("en_us_twitter")
```

## Summary

Tablets and smartphones are the dominate means for electronic communication across the globe.
Unfortunately, while we benefit from lower prices and increased portability, we also suffer from hunting and pecking with our thumbs over tiny keyboards to produce even the simplest of text messages.
This project will address this issue by providing the user with an augmented keyboard that saves them from typing every word with an automatic, predictive heads-up next-word selection mechanism.

Our implementation will use a [probabilistic n-gram model](https://en.wikipedia.org/wiki/N-gram) to accurately recommend follow-on words.
This will keep the memory and processor requirements low enough to fit on portable devices while being quick and accurate enough to keep the users happy.
The goal of the preliminary investigation is to look at trends in the data which will influence our application design.
The remainder of this report describes a single source of English text across three genres that will serve as the basis for the application prototype; in particular, three takeaways from the the data are

1. Users writing news, blogs, or twitter messages have considerably different needs
1. The data is sparse, very sparse, so we will need to [smooth](https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques) probabilities in our model
1. Even within genre, the out-of-vocabulary rate is high

The initial prototype application will involve developing a straightforward trigram model per genre with backoff that will meet our specific processing and memory requirements.
From this we will explore options for improving accuracy within the resource bounds, making sure we do not end up with a highly accurate model that needs too much memory or processing power to run on a resource-constrained device.
The target environment for the early prototype is a Shiny app that will execute in a constrained environment of it's own.
This means we cannot rely on having lots of memory or cores for loading and processing large data sets.
And we should also not expect to be able to run many models in parallel, as we might do for a random forests, for instance.

## Sources and Genres

The English corpora from the [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) is used in this report.
The table below shows details for each source: blogs, news, and twitter.
Given the amount of data for each source we use only a sub-sample for this exploration.

```{r}
file_line_count <- function(filename) {
  paste0("wc -l ", filename) %>%
    system(intern = TRUE) %>%
    str_extract("\\d+") %>%
    as.integer()
}
```

```{r files_data, cache=TRUE}
files <- tribble(
  ~ raw_filename, ~ rda_filename,
  "../data-raw/final/en_US/en_US.blogs.txt", "../data/en_us_blogs.rda",
  "../data-raw/final/en_US/en_US.news.txt", "../data/en_us_news.rda",
  "../data-raw/final/en_US/en_US.twitter.txt", "../data/en_us_twitter.rda"
) %>%
  mutate(raw_size = file.size(raw_filename),
         raw_line_count = map_int(raw_filename, file_line_count),
         rda_size = file.size(rda_filename))
```

```{r raw_data_table}
files %>%
  select(-rda_filename) %>%
  mutate(raw_filename = str_replace(raw_filename, "../data-raw/final/en_US/", "")) %>%
  rename(`File name` = raw_filename,
         `File size (B)` = raw_size,
         `Line count` = raw_line_count,
         `Sub-sample size (B)` = rda_size) %>%
  knitr::kable(format.args = list(big.mark = ","))
```

```{r texts_data, cache=TRUE, dependson=c("files_data")}
texts <- tribble(
  ~ source, ~ corpora,
  "Blogs", en_us_blogs,
  "News", en_us_news,
  "Twitter", en_us_twitter
) %>%
  mutate(num_obs = map_int(corpora, nrow),
         words = map(corpora, ~ unnest_tokens(.x, word, text)),
         total_words = map_int(words, nrow),
         unique_words = map_int(words, ~ length(unique(.x[["word"]]))))
```

The following table shows more statistics for each of the sub-sampled sources.
One point to note is that since we sample lines the Twitter data set has less than half the number of total words and just around half the number of distinct words as the news and blogs source, which makes sense considering the limited number of characters allowed in a tweet.

```{r text_data_table}
texts %>%
  select(-corpora, -words) %>%
  knitr::kable(col.names = c("Source", "Line count", "Total word count", "Distinct word count"),
               format.args = list(big.mark = ","))
```

These sizes demonstrate that a look-up approach is untenable and that we need a space-efficient model.

```{r}
texts <- tribble(
  ~ source, ~ corpora,
  "Blogs", en_us_blogs,
  "News", en_us_news,
  "Twitter", en_us_twitter
) %>%
  mutate(split = map(corpora, ~ enframe(modelr::resample_partition(.x, p = c(train = 0.80, validate = 0.10, test = 0.10)))  %>% spread(name, value))) %>%
  unnest(split) %>%
  mutate(words = map(train, ~ unnest_tokens(as.data.frame(.x), word, text)))
```

## Frequent Words

As we might expect, only a relatively small number number or words are used frequently and most are words are used very infrequently.
The following figure shows the frequency distribution of words with respect to each source.
Note that the distributions for each source are very similar.

```{r cache=TRUE, dependson=c("texts_data")}
texts %>%
  select(source, words) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  ggplot(aes(n)) +
  geom_density(aes(color = source)) +
  scale_x_log10() +
  scale_color_discrete("Source") +
  labs(title = "Word frequency distributions are extremely left-skewed",
       x = "", y = "")
```

Given this distribution, it might be beneficial to look at the top-25 most frequent words per source.
The following figure combines the top-25 lists for each genre of text and shows the proportion of the sources that each word accounts for.
The words are ordered based on the ranking for news, allowing us to compare that trend with the other sources.
At the top of the list for all three sources are the expected words like "the", "and", "a", etc.
However, what this side-by-side comparison helps highlight is that while the frequency distributions for each source are very similar, the most frequent words are not.
Both blogs and Twitter text have significantly higher use of personal pronouns, such as "I" and "you", than are seen in news.
This is intuitive, since blogs and twitter are more personal and direct communication mediums, and a good indicator that we should model these different sources differently.

```{r cache=TRUE, dependson=c("texts_data")}
top_words <- texts %>%
  select(source, words) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  top_n(25, n) %>%
  ungroup() %>%
  distinct(word)

texts %>%
  select(source, words) %>%
  mutate(source = factor(source, levels = c("News", "Blogs", "Twitter"), ordered = TRUE)) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  mutate(prop = n / sum(n)) %>%
  inner_join(top_words, by = "word") %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, prop)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  facet_wrap(~ source) +
  labs(title = "Per-source proportion of top-25 most frequent words",
       x = "", y = "Proportion of text")
```

The following table shows another indication of the cross-genre differences in terms of the correlation of word use.
We see that the news and blog corpora are most alike and that the news and twitter corpora for are least similar.

```{r}
tidy_freqs <- texts %>%
  select(source, words) %>%
  mutate(source = factor(source, levels = c("News", "Blogs", "Twitter"), ordered = TRUE)) %>%
  unnest(words) %>%
  count(source, word, sort = TRUE) %>%
  mutate(frequency = n / sum(n)) %>%
  ungroup() %>%
  filter(str_detect(word, "['a-z]+")) %>%
  select(-n)
```

```{r}
cor_news_blogs <- tidy_freqs %>%
  spread(source, frequency) %>%
  cor.test(~ News + Blogs, data = ., na.action = "na.omit")
```

```{r}
cor_news_twitter <- tidy_freqs %>%
  spread(source, frequency) %>%
  cor.test(~ News + Twitter, data = ., na.action = "na.omit")
```

```{r}
cor_blogs_twitter <- tidy_freqs %>%
  spread(source, frequency) %>%
  cor.test(~ Blogs + Twitter, data = ., na.action = "na.omit")
```

```{r}
bind_rows(
  mutate(broom::tidy(cor_news_blogs), pair = "News-Blogs"),
  mutate(broom::tidy(cor_news_twitter), pair = "News-Twitter"),
  mutate(broom::tidy(cor_blogs_twitter), pair = "Blogs-Twitter")
) %>%
  select(pair, estimate, conf.low, conf.high) %>%
  arrange(desc(estimate)) %>%
  knitr::kable(digits = 3, col.names = c("Correlation Test", "Estmiate", "Lower 95% CI", "Upper 95% CI"))
```

## Sparse Word Pairs

The previous section looked at the distributions of words and how the different genres differ at the word level.
We now want to investigate word pairings, or bigrams.
Of particular interest is understanding how sparse the data set is as this will effect both how we handle smoothing of the data and how we represent the model in a space-efficient manner.

This figure shows which of all possible pairs of the top-25 words from the News set are connected.
Even given that these are mostly common articles and conjunctions we see the data set is rather sparse.

```{r}
top_news_words <- texts %>%
  filter(source == "News") %>%
  select(words) %>%
  unnest(words) %>%
  count(word, sort = TRUE) %>%
  top_n(25, n)

news_bigrams <- texts %>%
  filter(source == "News") %>%
  select(corpora) %>%
  unnest(corpora) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE) %>%
  separate(bigram, c("left", "right"), sep = " ")

news_bigrams %>%
  filter(left %in% top_news_words$word) %>%
  filter(right %in% top_news_words$word) %>%
  ggplot(aes(left, right)) +
  geom_tile(show.legend = FALSE) +
  coord_flip() +
  labs(x = "First word", y = "Second word") +
  theme(axis.ticks = element_blank())
```

The next figure "zooms out" to show which pairs exist for (only) the top-500 words in the News source.
In this case, we show the adjacency matrices for each genre.

```{r}
top_news_words <- texts %>%
  filter(source == "News") %>%
  select(words) %>%
  unnest(words) %>%
  count(word, sort = TRUE) %>%
  top_n(500, n)

all_bigrams <- texts %>%
  select(source, corpora) %>%
  unnest(corpora) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(source, bigram, sort = TRUE) %>%
  separate(bigram, c("left", "right"), sep = " ")

all_bigrams %>%
  filter(left %in% top_news_words$word) %>%
  filter(right %in% top_news_words$word) %>%
  ggplot(aes(left, right, fill = source)) +
  geom_tile() +
  facet_wrap(~ source) +
  labs(x = "First word", y = "Second word") +
  coord_flip() +
  guides(fill = FALSE) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        aspect.ratio = 1.0)
```

From this it's clear that we have high sparsity in each genre.

## Unknown Words

We now assess the out-of-vocabulary (OOV) rate.
The corpora for each genre was split, 80% for training, 10% for validation, and 10% for the final model testing.
An OOV set was computed as the set of unique words in the validation dataset but not in the training dataset.
From this we computed the OOV rate as the percentage of OOV in the validation dataset.
The following table shows the rates for each genre.

```{r}
oov <- texts %>%
  mutate(unique_train_words = map_int(words, ~ length(unique(.x[["word"]]))),
         validate_words = map(validate, ~ unnest_tokens(as.data.frame(.x), word, text)),
         unique_validate_words = map_int(validate_words, ~ length(unique(.x[["word"]]))),
         oov = setdiff(unique_validate_words, unique_train_words),
         oov_rate  = length(oov) / unique_validate_words)
oov <- texts %>%
  select(-corpora, -test) %>%
  mutate(validate_words = map(validate, ~ unnest_tokens(as.data.frame(.x), word, text)),
         distinct_train_words = map(words, ~ distinct(.x, word)),
         distinct_validate_words = map(validate_words, ~ distinct(.x, word)),
         oov_words = map2(distinct_validate_words, distinct_train_words, anti_join, by = "word"),
         oov_rate = map_int(oov_words, nrow) / map_int(distinct_validate_words, nrow))
```

```{r}
oov %>%
  select(source, oov_rate) %>%
  knitr::kable(digits = 3, col.names = c("Source", "OOV Rate"))
```

As we see, even in the relatively small hold-out, around 25% of the words do not exist in the training data.
That means it's likely that our application will have to handle a significant number of words that were not used when training the model.
