---
title: "YM-KN"
author: "Dylan Stark"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{YM-KN}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
library(tidyverse)
library(stringr)
library(tidytext)

library(courseraswiftkey)
```

```{r}
data("en_us_news")

ex_en <- en_us_news %>%
  head(100)
```

Implementation of [Kneser-Ney smoothing](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing).

Discount: $\delta$

```{r}
delta <- 0.80
```

Vocab. with "_s_unk".

```{r}
vocab <- ex_en %>%
  unnest_tokens(word, text) %>%
  distinct(word) %>%
  rbind(data.frame(word = c("_s_unk")))
vocab
```

## Unigram

```{r}
uni_en <- ex_en %>%
  unnest_tokens(pair, text, token = "ngrams", n = 2) %>%
  mutate(prefix = word(pair, 1),
         word = word(pair, -1)) %>%
  select(-line, -pair)
uni_en
```

Continuation counts

```{r}
cont_counts <- vocab %>%
  left_join(uni_en, by = c("word")) %>%
  distinct(prefix, word) %>%
  mutate(cont_count = ifelse(is.na(prefix), 0, 1)) %>%
  group_by(word) %>%
  summarize(num_distinct_context = sum(cont_count))
cont_counts %>%
  arrange(desc(num_distinct_context))

#cont_counts %>%
#  filter(num_distinct_context > delta) %>%
#  left_join(uni_num_distinct_pairs, by = c("word")) %>%
#  mutate(x = delta * num_distinct_context / num_distinct_pairs) %>%
#  summarize(y = sum(x))
```



Number of distinct contexts this word occurs in: $|\{w\prime : 0 < c(w\prime, w_i)\}|$

```{r}
#uni_distinct_context <- uni_en %>%
#  distinct(prefix, word) %>%
#  group_by(word) %>%
#  summarize(num_distinct_context = n())
#uni_distinct_context %>%
#  arrange(desc(num_distinct_context))
```

Number of distinct pairs of words: $|\{(w\prime, w\prime\prime) : 0 < c(w\prime, w\prime\prime)\}|$

```{r}
uni_num_distinct_pairs <- vocab %>%
  left_join(uni_en, by = c("word")) %>%
  distinct(prefix, word) %>%
  mutate(count = ifelse(is.na(prefix), 0, 1)) %>%
  mutate(num_distinct_pairs = sum(count)) %>%
  distinct(word, num_distinct_pairs) %>%
  select(word, num_distinct_pairs)
uni_num_distinct_pairs
```

Discount we need to spread: $\lambda(\epsilon)$

```{r}
lambda_epsilon <- vocab %>%
  left_join(cont_counts, by = c("word")) %>%
  left_join(uni_num_distinct_pairs, by = c("word")) %>%
  mutate(discount = ifelse(num_distinct_context > delta, delta, 0),
         p_kn_uni = discount / num_distinct_pairs) %>%
  distinct(word, p_kn_uni)  %>%
  mutate(lambda_epsilon = sum(p_kn_uni)) %>%
  select(word, lambda_epsilon)
lambda_epsilon
```

Unigram probability: $$p_{KN}(w_i) = \frac{|\{w\prime : 0 < c(w\prime, w_i)\}|}{|\{(w\prime, w\prime\prime) : 0 < c(w\prime, w\prime\prime)\}|}$$

```{r}
#p_kn_uni_works <- uni_en %>%
#  left_join(uni_distinct_context, by = c("word")) %>%
#  left_join(uni_num_distinct_pairs, by = c("word")) %>%
#  mutate(p_kn_uni = num_distinct_context / num_distinct_pairs) %>%
#  distinct(word, p_kn_uni) %>%
#  select(word, p_kn_uni)

# New
#lambda_epsilon <- 0.4149445 #1/2 - 1/8 + 1/16 - 1/32
V <- nrow(vocab)

p_kn_uni <- vocab %>%
  left_join(cont_counts, by = c("word")) %>%
  left_join(uni_num_distinct_pairs, by = c("word")) %>%
  left_join(lambda_epsilon, by = c("word")) %>%
  mutate(discount = pmax(num_distinct_context - delta, 0),
         p_kn_uni = discount / num_distinct_pairs + (lambda_epsilon / V)) %>%
  distinct(word, p_kn_uni)
p_kn_uni
```

Tests for true probability distribution:

```{r}
#p_kn_uni_works %>%
#  summarize(total_p = sum(p_kn_uni))
p_kn_uni %>%
  summarize(total_p = sum(p_kn_uni))
```

```{r}
p_kn_uni %>%
  filter(p_kn_uni > 1)
```

## Bigram

```{r}
bi_en <- ex_en %>%
  unnest_tokens(bi, text, token = "ngrams", n = 2) %>%
  mutate(prefix = word(bi, 1),
         word = word(bi, -1)) %>%
  select(-line, -bi)
bi_en
```

```{r}
bi_discounted_counts <- bi_en %>%
  group_by(prefix, word) %>%
  summarize(c_prefix_word = n()) %>%
  mutate(max_prefix_word = pmax(c_prefix_word - delta, 0)) %>%
  select(-c_prefix_word)
bi_discounted_counts %>%
  arrange(prefix, word)
```

```{r}
bi_all_c_prefix_any <- bi_en %>%
  group_by(prefix) %>%
  summarize(c_prefix_any = n())
bi_all_c_prefix_any %>%
  arrange(desc(c_prefix_any), prefix)
```

```{r}
bi_distinct_followers <- bi_en %>%
  distinct(prefix, word) %>%
  group_by(prefix) %>%
  summarize(num_distinct_followers = n())
bi_distinct_followers %>%
  arrange(desc(num_distinct_followers), prefix)
```

```{r}
p_kn_bi <- bi_en %>%
  left_join(bi_discounted_counts, by = c("prefix", "word")) %>%
  left_join(bi_all_c_prefix_any, by = c("prefix")) %>%
  left_join(bi_distinct_followers, by = c("prefix")) %>%
  left_join(p_kn_uni, by = c("word")) %>%
  mutate(first_term = max_prefix_word / c_prefix_any,
         norm_discount = delta / c_prefix_any,
         num_discounted =  num_distinct_followers,
         normalizing_constant = norm_discount * num_discounted,
         p_kn_bi = first_term + normalizing_constant * p_kn_uni) %>%
  distinct(prefix, word, p_kn_bi, first_term, normalizing_constant, p_kn_uni, norm_discount, num_discounted)
p_kn_bi
```

```{r}
ex_en %>%
  head(1)
p_kn_bi %>%
  unite(phrase, prefix, word, sep = " ")
```

```{r}
p_kn_bi %>%
  filter(prefix == "historically") %>%
  select(prefix, word, p_kn_bi) %>%
  arrange(desc(p_kn_bi))
```

```{r}
p_kn_bi %>%
  filter(prefix == "they've") %>%
  select(prefix, word, p_kn_bi) %>%
  arrange(desc(p_kn_bi))
```

```{r}
p_kn_bi %>%
  filter(prefix == "been") %>%
  select(prefix, word, p_kn_bi) %>%
  arrange(desc(p_kn_bi))
```

Tests for true probability distribution:

```{r}
p_kn_bi %>%
  group_by(word) %>%
  summarize(p_word = prod(p_kn_bi)) %>%
  summarize(total_p = sum(p_word))
```

```{r}
summary(p_kn_bi)
```

```{r}
p_kn_bi %>%
  filter(p_kn_bi > 1)
```

## Trigram

```{r}
tri_en <- ex_en %>%
  unnest_tokens(tri, text, token = "ngrams", n = 3) %>%
  mutate(prefix = word(tri, 1, 2),
         bi = word(tri, 2),
         word = word(tri, -1)) %>%
  select(-line, -tri)
tri_en
```

```{r}
tri_discounted_counts <- tri_en %>%
  group_by(prefix, word) %>%
  summarize(c_prefix_word = n()) %>%
  mutate(max_prefix_word = pmax(c_prefix_word - delta, 0)) %>%
  select(-c_prefix_word)
tri_discounted_counts %>%
  arrange(prefix, word)
```

```{r}
tri_all_c_prefix_any <- tri_en %>%
  group_by(prefix) %>%
  summarize(c_prefix_any = n()) %>%
  arrange(desc(c_prefix_any), prefix)
tri_all_c_prefix_any
```

```{r}
tri_distinct_followers <- tri_en %>%
  distinct(prefix, word) %>%
  group_by(prefix) %>%
  summarize(num_distinct_followers = n())
tri_distinct_followers %>%
  arrange(desc(num_distinct_followers), prefix)
```

```{r}
tri_occurrences <- tri_en %>%
  group_by(prefix, word) %>%
  summarize(sum_c_tris = n())
tri_occurrences %>%
  arrange(desc(sum_c_tris))
```

```{r}
p_kn_tri <- tri_en %>%
  left_join(tri_discounted_counts, by = c("prefix", "word")) %>%
  left_join(tri_all_c_prefix_any, by = c("prefix")) %>%
  left_join(tri_distinct_followers, by = c("prefix")) %>%
  left_join(tri_occurrences, by = c("prefix", "word")) %>%
  left_join(p_kn_bi, by = c("bi" = "prefix", "word")) %>%
  mutate(first_term = max_prefix_word / c_prefix_any,
         second_term = delta * (num_distinct_followers / sum_c_tris),
         p_kn_tri = first_term + second_term * p_kn_bi) %>%
  select(prefix, word, p_kn_bi, p_kn_tri)
p_kn_tri
```

## Quadgram

```{r}
quad_en <- ex_en %>%
  unnest_tokens(quad, text, token = "ngrams", n = 4) %>%
  mutate(prefix = word(quad, 1, 3),
         bi = word(quad, 2, 3),
         word = word(quad, -1)) %>%
  select(-line, -quad)
quad_en
```

Discounted count of times word follows prefix: $max(c(w_{i-n+1}^{i-1}, w_i) - \delta, 0)$

```{r}
quad_discounted_counts <- quad_en %>%
  group_by(prefix, word) %>%
  summarize(c_prefix_word = n()) %>%
  mutate(max_prefix_word = pmax(c_prefix_word - delta, 0)) %>%
  select(-c_prefix_word)
quad_discounted_counts %>%
  arrange(prefix, word)
```

Sum of counts of times any word follows prefix: $\sum_{w\prime}c(w_{i-1},w\prime)$

```{r}
quad_all_c_prefix_any <- quad_en %>%
  group_by(prefix) %>%
  summarize(c_prefix_any = n()) %>%
  arrange(desc(c_prefix_any), prefix)
quad_all_c_prefix_any
```

Number of distinct words occurring after some prefix: $|\{w\prime : 0 < c(w_{i-n+1}^{i-1}, w\prime)\}|$

```{r}
quad_distinct_followers <- quad_en %>%
  distinct(prefix, word) %>%
  group_by(prefix) %>%
  summarize(num_distinct_followers = n())
quad_distinct_followers %>%
  arrange(desc(num_distinct_followers), prefix)
```

Number of times quad appears (per word): $\sum_{w_i}c(w_{i-n+1}^{i-1})$

```{r}
quad_occurrences <- quad_en %>%
  group_by(prefix, word) %>%
  summarize(sum_c_quads = n())
quad_occurrences %>%
  arrange(desc(sum_c_quads))
```

Kneser-Ney smoothed probability: $$p_{KN}(w_i | w_{i-n+1}^{i-1}) = \frac{max(c(w_{i-n+1}^{i-1}, w_i) - \delta, 0)}{\sum_{w\prime}c(w_{i-1},w\prime)} + \delta\frac{|\{w\prime : 0 < c(w_{i-n+1}^{i-1}, w\prime)\}|}{\sum_{w_i}c(w_{i-n+1}^{i})}$$

```{r}
p_kn_quad <- quad_en %>%
  left_join(quad_discounted_counts, by = c("prefix", "word")) %>%
  left_join(quad_all_c_prefix_any, by = c("prefix")) %>%
  left_join(quad_distinct_followers, by = c("prefix")) %>%
  left_join(quad_occurrences, by = c("prefix", "word")) %>%
  left_join(p_kn_tri, by = c("bi" = "prefix", "word")) %>%
  mutate(first_term = max_prefix_word / c_prefix_any,
         second_term = delta * (num_distinct_followers / sum_c_quads),
         p_kn_quad = first_term + second_term * p_kn_tri) %>%
  select(prefix, bi, word, p_kn_tri, p_kn_quad)
p_kn_quad
```


