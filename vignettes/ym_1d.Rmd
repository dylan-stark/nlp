---
title: "YM-1D.Rmd"
author: "Dylan Stark"
date: "3/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidytext)
library(modelr)

library(courseraswiftkey)
```

YM-1D is a 4-gram language model.
This report includes evaluations of uni-, bi-, and tri-grams, also.
All models are trained and evaluated against the each US English corpus separately.

## Data

Load available data.

```{r}
data("en_us_news")
data("en_us_twitter")
data("en_us_blogs")
```

Generate train-test-validate split.
Use `down_sample` rate to tune amount of corpus data to use.
Table (`tbl`) contains original data, sample, and randomized partitions.

```{r}
set.seed(42)

down_sample <- 0.01

# Partition data frame into random 80:10:10 split and return as a wide
# data frame (tibble).
spread_resample <- function(data) {
  data  %>%
    resample_partition(c(train = 0.80, validate = 0.10, test = 0.10)) %>%
    enframe() %>%
    spread(name, value)
}

tbl <- tribble(
  ~desc, ~data,
  "news", en_us_news,
  "blogs", en_us_blogs,
  "twitter", en_us_twitter
) %>%
  mutate(
    sample = map(data, sample_frac, down_sample),
    resample = map(sample, spread_resample)
  ) %>%
  unnest(resample)
```

## Unigram Model

The unigram model selects the next word from the set of known words with probability proportional to how frequent the word appeared in the training corpus.

```{r}
unigram_tbl <- tbl %>%
  select(desc, train, validate) %>%
  mutate(
    train = map(train, as.data.frame),
    validate = map(validate, as.data.frame),
    unigram_model = map(train, build_unigram_model, k = 0.001)
  )
```

### Example suggestions

Example suggestions from news corpus.

```{r}
unigram_tbl[1, "unigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

Example suggestions from blogs corpus.

```{r}
unigram_tbl[2, "unigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

Example suggestions from twitter corpus.

```{r}
unigram_tbl[3, "unigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

### Evaluation

For each word in the validation set, predict the word using the unigram model.
This is equivalent to generating a sample from the model that is the same size as the validation set (word count).

Accuracy is measured as the proportion of correct predictions.

```{r}
predict_unigram_top_n <- function(word, model, n) {
  model %>%
    arrange(desc(prob)) %>%
    head(n) %>%
    sample_n(1, weight = prob) %>%
    .[[1, "w1"]]
}

predict_unigram_weighted <- function(word, model, n) {
  predict_unigram_top_n(word, model, n = nrow(model))
}

predict_unigram_top <- function(word, model, n) {
  predict_unigram_top_n(word, model, n = 1)
}

predict <- function(model, data, .f, n) {
  data %>%
    unnest_tokens(w1, text) %>%
    mutate(y1 = map_chr(w1, .f, model = model, n = n),
           match = w1 == y1) %>%
    summarize(accuracy = sum(match) / n()) %>%
    .[["accuracy"]]
}
```

```{r}
#unigram_tbl %>%
#  mutate(weighted_acc = map2_dbl(unigram_model, validate, predict, predict_unigram_weighted),
#         top_acc = map2_dbl(unigram_model, validate, predict, predict_unigram_top),
#         top_10_acc = map2_dbl(unigram_model, validate, predict, predict_unigram_top_n, n = 10)) %>%
#  select(-train, -validate, -unigram_model)
```

Choosing the top most-frequent word from the training set is more accurate than randomly choosing from a weighted distribution, though that accuracy is very bad.


## Bigram Model

The bigram model selects the next word based on preceding word.

We add a `_s_start` token to the beginning of each entry in the training dataset to caputure the beginning word in a `(_s_start, <word>)` tuple.

We also insert `_s_unk` tokens so that our model can work with out-of-vocabulary words (i.e., words not seen in the training data set).
We have to do this because our model uses one word-worth of context.

```{r}
add_start_token <- function(data, start = "_s_start", n = 1) {
  data %>%
    mutate(text = paste(paste(rep(start, n), collapse = " "), text))
}

add_stop_token <- function(data, stop = "_s_stop", n = 1) {
  data %>%
    mutate(text = paste(text, paste(rep(stop, n), collapse = " ")))
}

add_unknowns <- function(data, symbol = "_s_unk") {
  data %>%
    unnest_tokens(word, text) %>%
    group_by(word) %>%
    mutate(word2 = lag(word, n = 1, default = symbol)) %>%
    group_by(line) %>%
    summarize(text = paste(word2, collapse = " "))
}

count_bigrams <- function(data) {
  data %>%
    unnest_tokens(pair, text, token = "ngrams", n = 2, collapse = FALSE) %>%
    separate(pair, into = c("w1", "w2"), sep = " ", remove = FALSE) %>%
    count(w1, w2) %>%
    ungroup()
}

build_bigram_model <- function(data, k = 1.0) {
  data <- data %>%
    add_start_token() %>%
    add_unknowns()
  
  word_counts <- count_words(data)
  bigram_counts <- count_bigrams(data)
  
  V <- nrow(word_counts)
  
  bigram_counts %>%
    inner_join(word_counts, by = c("w1")) %>%
    rename(c1 = n.y, c2 = n.x) %>%
    mutate(prob = (c2 + k) / (c1 + k * V)) %>%
    ungroup()
}

bigram_tbl <- tbl %>%
  select(desc, train, validate) %>%
  mutate(
    train = map(train, as.data.frame),
    validate = map(validate, as.data.frame),
    bigram_model = map(train, build_bigram_model, k = 0.001)
  )
```

### Example suggestions

Example suggestions from news corpus.

```{r}
bigram_tbl[1, "bigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

Example suggestions from blogs corpus.

```{r}
bigram_tbl[2, "bigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

Example suggestions from news corpus.

```{r}
bigram_tbl[1, "bigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```


### Evaluation

For each pair of words (including the `(_s_start, <word>)` tuple) in the validation set, predict the second word using the bigram model.

Accuracy is measured as the proportion of correct predictions.

```{r}
recommend_bigram <- function(model, word, n = 1) {
  exact_match <- model %>%
    filter(w1 == word & w2 != "_s_unk") %>%
    arrange(desc(prob))
  if (nrow(exact_match) > 0) {
    return(exact_match[1:n, "w2"][["w2"]])
  }
  
  unk_match <- model %>%
    filter(w1 == '_s_unk' & w2 != "_s_unk") %>%
    arrange(desc(prob))
  return(unk_match[1:n, "w2"][["w2"]])
}

predict_bigram <- function(model, data) {
  data %>%
    add_start_token() %>%
    unnest_tokens(pair, text, token = "ngrams", n = 2) %>%
    separate(pair, into = c("w1", "w2"), sep = " ") %>%
    mutate(y2 = map_chr(w1, recommend_bigram, model = model),
           match = w2 == y2) %>%
    summarize(accuracy = sum(match) / n()) %>%
    .[["accuracy"]]
}

```

Test:

```{r}
recommend_bigram(bigram_tbl[[1, "bigram_model"]], "", 10)
```

```{r}
recommend_bigram(bigram_tbl[[1, "bigram_model"]], "because", 10)
```

Sweep:

```{r}
#bigram_tbl %>%
#  mutate(acc = map2_dbl(bigram_model, validate, predict_bigram)) %>%
#  select(-train, -validate, -bigram_model)
```

Accuracy is still fairly poor across the board.

### Save

```{r}
ym_1b_news <- bigram_tbl[[1, "bigram_model"]]
write_rds(ym_1b_news, path = "ym_1b_news.rds")
```

### Perplexity

```{r}
ym_1b_news_unigram <- unigram_tbl[[1, "unigram_model"]]

compute_perplexity <- function(unigram_model, model, data) {
  data <- data %>%
    add_start_token() %>%
    add_stop_token() %>%
    summarize(text = paste(text, collapse = " "))
  #return(data)
  
  # Compute bigram probabilities
  bigrams <- data %>%
    #add_start_token() %>%
    unnest_tokens(pair, text, token = "ngrams", n = 2) %>%
    separate(pair, into = c("w1", "w2"), sep = " ") %>%
    filter(!(w1 == "_s_stop" & w2 == "_s_start")) %>%
    left_join(model, by = c("w1", "w2"))
  #return(bigrams)

  N <- nrow(bigrams) 
  
  # If some bigrams didn't match, use unigrams 
  unigrams <- bigrams %>%
    filter(is.na(prob)) %>%
    select(w2) %>%
    left_join(unigram_model, by = c("w2" = "w1"))
  #return(unigrams)

  # Default to unigram unknown prob.
  unknowns <- unigrams %>%
    filter(is.na(prob)) %>%
    mutate(w1 = "_s_unk") %>%
    left_join(unigram_model, by = c("w1" = "w1"))
  #return(unknowns)
    
  all_probs <- rbind(
    bigrams %>%
      filter(!is.na(prob)) %>%
      select(prob),
    unigrams %>%
      filter(!is.na(prob)) %>%
      select(prob),
    unknowns %>%
      select(prob.y) %>%
      rename(prob = prob.y)
  )
  #return(all_probs)
 
  all_probs %>% 
    mutate(log_prob = log2(prob)) %>%
    summarize(n = n(),
              sum_log_probs = sum(log_prob)) %>%
    mutate(perp = 2^(-(1/n) * sum_log_probs))
}
data.frame(text = c("I want to go fishing",
                    "don't stay too long",
                    "I can't believe you")) %>%
  compute_perplexity(ym_1b_news_unigram, ym_1b_news, .)
data.frame(text = c("I want to go fishing")) %>%
  compute_perplexity(ym_1b_news_unigram, ym_1b_news, .)
data.frame(text = c("don't go swimming")) %>%
  compute_perplexity(ym_1b_news_unigram, ym_1b_news, .)
data.frame(text = c("to on for fast not")) %>%
  compute_perplexity(ym_1b_news_unigram, ym_1b_news, .)

#unigram_tbl[[1, "validate"]] %>%
unigram_tbl[[1, "train"]] %>%
  compute_perplexity(ym_1b_news_unigram, ym_1b_news, .)
```

```{r}
ym_1b_news_unigram %>%
  filter(w1 == "_s_unk")
```

## Trigram Model

The trigram model selects the next word based on preceding two word.

We add a two `_s_start` tokens to the beginning of each entry in the training dataset to caputure the beginning word in a `(_s_start, _s_start, <word>)` tuple.

We also insert `_s_unk` tokens so that our model can work with out-of-vocabulary words (i.e., words not seen in the training data set).
We have to do this because our model uses one word-worth of context.

```{r}
count_trigrams <- function(data) {
  data %>%
    unnest_tokens(triples, text, token = "ngrams", n = 3, collapse = FALSE) %>%
    separate(triples, into = c("w1", "w2", "w3"), sep = " ", remove = FALSE) %>%
    count(w1, w2, w3) %>%
    ungroup()
}

build_trigram_model <- function(data, k = 1.0) {
  data <- data %>%
    add_start_token(n = 2) %>%
    add_unknowns()
  
  bigram_counts <- count_bigrams(data)
  trigram_counts <- count_trigrams(data)
  
  V <- nrow(bigram_counts)
  
  trigram_counts %>%
    inner_join(bigram_counts, by = c("w1", "w2")) %>%
    rename(c1 = n.y, c2 = n.x) %>%
    mutate(prob = (c2 + k) / (c1 + k * V)) %>%
    ungroup()
}

trigram_tbl <- tbl %>%
  select(desc, train, validate) %>%
  mutate(
    train = map(train, as.data.frame),
    validate = map(validate, as.data.frame),
    trigram_k1_model = map(train, build_trigram_model, k = 1),
    trigram_model = map(train, build_trigram_model, k = 0.001),
    trigram_k0_model = map(train, build_trigram_model, k = 0)
  )
```

### Example suggestions

Example suggestions from news corpus.

```{r}
trigram_tbl[1, "trigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

Example suggestions from blogs corpus.

```{r}
trigram_tbl[2, "trigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

Example suggestions from news corpus.

```{r}
trigram_tbl[1, "trigram_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

```{r}
trigram_tbl[1, "trigram_k0_model"][[1, 1]] %>%
  sample_n(3, weight = prob) %>%
  arrange(desc(prob))
```

### Evaluation

For each triple of words (including the `(_s_start, _s_start, <word>)` 3-tuple) in the validation set, predict the third word using the bigram model.

Accuracy is measured as the proportion of correct predictions.

```{r}
predict_trigram <- function(model, data) {
  predict_trigram_ <- function(model, word1, word2) {
    exact_match <- model %>%
      filter(w1 == word1 & w2 == word2 & w3 != "_s_unk") %>%
      arrange(desc(prob))
    if (nrow(exact_match) > 0) {
      return(exact_match[[1, "w3"]])
    }
    
    unk_match <- model %>%
      filter(w1 == '_s_unk' & w2 == '_s_unk' & w3 != '_s_unk') %>%
      arrange(desc(prob))
    return(unk_match[[1, "w3"]])
  }
  
  data %>%
    add_start_token(n = 2) %>%
    unnest_tokens(triple, text, token = "ngrams", n = 3) %>%
    separate(triple, into = c("w1", "w2", "w3"), sep = " ") %>%
    mutate(y3 = map2_chr(w1, w2, predict_trigram_, model = model),
           match = w3 == y3) %>%
    summarize(accuracy = sum(match) / n()) %>%
    .[["accuracy"]]
}
```


```{r}
trigram_tbl %>%
  mutate(acc_k_001 = map2_dbl(trigram_model, validate, predict_trigram),
         acc_k_0 = map2_dbl(trigram_k0_model, validate, predict_trigram),
         acc_k_1 = map2_dbl(trigram_k1_model, validate, predict_trigram)) %>%
  select(-train, -validate, -trigram_model, -trigram_k0_model)
```

## Backoff Model

```{r}
build_backoff_model <- function(data) {
  unigram_model <- build_unigram_model(data)
  bigram_model <- build_bigram_model(data, k = 0.001)
  trigram_model <- build_trigram_model(data, k = 0.001)
 
  list(
    "unigram_model" = unigram_model,
    "bigram_model" = bigram_model,
    "trigram_model" = trigram_model
  )
}

backoff_tbl <- tbl %>%
  select(desc, train, validate) %>%
  mutate(
    train = map(train, as.data.frame),
    validate = map(validate, as.data.frame),
    backoff_model = map(train, build_backoff_model)
  )
backoff_tbl
```

```{r}
predict_backoff <- function(model, data) {
  predict_backoff_ <- function(model, word1, word2) {
    exact_match <- model %>%
      filter(w1 == word1 & w2 == word2 & w3 != "_s_unk") %>%
      arrange(desc(prob))
    if (nrow(exact_match) > 0) {
      return(exact_match[[1, "w3"]])
    }
    
    unk_match <- model %>%
      filter(w1 == '_s_unk' & w2 == '_s_unk' & w3 != '_s_unk') %>%
      arrange(desc(prob))
    return(unk_match[[1, "w3"]])
  }
  
  data %>%
    add_start_token(n = 2) %>%
    unnest_tokens(triple, text, token = "ngrams", n = 3) %>%
    separate(triple, into = c("w1", "w2", "w3"), sep = " ") %>%
    mutate(y3 = map2_chr(w1, w2, predict_trigram_, model = model),
           match = w3 == y3) %>%
    summarize(accuracy = sum(match) / n()) %>%
    .[["accuracy"]]
}
```


```{r}
trigram_tbl %>%
  mutate(acc_k_001 = map2_dbl(trigram_model, validate, predict_trigram),
         acc_k_0 = map2_dbl(trigram_k0_model, validate, predict_trigram),
         acc_k_1 = map2_dbl(trigram_k1_model, validate, predict_trigram)) %>%
  select(-train, -validate, -trigram_model, -trigram_k0_model)
```
